{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started - Distributed Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "#   You may not use this file except in compliance with the License.\n",
    "#   A copy of the License is located at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   or in the \"license\" file accompanying this file. This file is distributed\n",
    "#   on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "#   express or implied. See the License for the specific language governing\n",
    "#   permissions and limitations under the License.\n",
    "# ==============================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The increase of training Bayesian probabilistic models will results in increase in size and data consumption, which could not fit in a single processor. The training time of the model will increase significantly with the size of consumption. Hence, MXFusion implemented Horovod to carry out distributed training on Bayesian probabilistic models, which could significantly decrease consumption from GPUs and training times.\n",
    "\n",
    "We provide an easy interface to perform distributed training in MXFusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example from Getting Started\n",
    "\n",
    "We can start with the same toy example from the [Getting Started](getting_started.ipynb) about estimating the mean and variance of a set of data. Again, we generate the same 100 data points with a given mean and variance following a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, initialize Horovod with <tt>hvd.init()</tt>. We also want to set the global context to GPU or CPU depends where the code is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import horovod.mxnet as hvd\n",
    "import mxnet as mx\n",
    "hvd.init()\n",
    "mx.context.Context.default_ctx = mx.gpu(hvd.local_rank()) if mx.test_utils.list_gpus() else mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code below is the same data and model defined from [Getting Started](getting_started.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "mean_groundtruth = 3.\n",
    "variance_groundtruth = 5.\n",
    "N = 100\n",
    "data = np.random.randn(N)*np.sqrt(variance_groundtruth) + mean_groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion import Variable, Model\n",
    "from mxfusion.components.variables import PositiveTransformation\n",
    "from mxfusion.components.distributions import Normal\n",
    "from mxfusion.common import config\n",
    "config.DEFAULT_DTYPE = 'float64'\n",
    "\n",
    "m = Model()\n",
    "m.mu = Variable()\n",
    "m.s = Variable(transformation=PositiveTransformation())\n",
    "m.Y = Normal.define_variable(mean=m.mu, variance=m.s, shape=(N,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow distributed training instead of single processor training, the inference class used would be <tt>DistributedGradBasedInference</tt>. Note that currently the code is not running distributed training in Horovod as we are still not running <tt>horovodrun</tt> command from our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxfusion.inference import DistributedGradBasedInference, MAP\n",
    "\n",
    "infr = DistributedGradBasedInference(inference_algorithm=MAP(model=m, observed=[m.Y]))\n",
    "infr.run(Y=mx.nd.array(data, dtype='float64'), learning_rate=0.1, max_iter=2000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After optimization, the estimated parameters are stored in an instance of the class <tt>InferenceParameters</tt>, which can be access from an <tt>Inference</tt> instance by <tt>infr.params</tt>.\n",
    "\n",
    "We collect the estimated mean and variance and compared with the generating parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_estimated = infr.params[m.mu].asnumpy()\n",
    "variance_estimated = infr.params[m.s].asnumpy()\n",
    "\n",
    "print('The estimated mean and variance: %f, %f.' % (mean_estimated, variance_estimated))\n",
    "print('The true mean and variance: %f, %f.' % (mean_groundtruth, variance_groundtruth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Traning on Bayesian model\n",
    "\n",
    "From the above example, we have done a maximum likelihood estimate from the observed data with distributed training. As our distributed training supports Bayesian model, now we can follow the second example of [Getting Started](getting_started.ipynb), which uses Bayesian inference to estimate how much our estimated parameters differs from the true parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model()\n",
    "m.mu = Normal.define_variable(mean=mx.nd.array([0], dtype='float64'), \n",
    "                              variance=mx.nd.array([100], dtype='float64'), shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxfusion.components.functions import MXFusionGluonFunction\n",
    "\n",
    "m.s_hat = Normal.define_variable(mean=mx.nd.array([5], dtype='float64'), \n",
    "                                 variance=mx.nd.array([100], dtype='float64'),\n",
    "                                 shape=(1,), dtype='float64')\n",
    "trans_mxnet = mx.gluon.nn.HybridLambda(lambda F, x: F.Activation(x, act_type='softrelu'))\n",
    "m.trans = MXFusionGluonFunction(trans_mxnet, num_outputs=1, broadcastable=True)\n",
    "m.s = m.trans(m.s_hat)\n",
    "m.Y = Normal.define_variable(mean=m.mu, variance=m.s, shape=(N,), dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.inference import create_Gaussian_meanfield\n",
    "\n",
    "q = create_Gaussian_meanfield(model=m, observed=[m.Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow distributed training instead of single processor training, the inference class used would be <tt>DistributedGradBasedInference</tt>. The default <tt>grad_loop</tt> of <tt>DistributedGradBasedInference</tt> is <tt>DistributedBatchInferenceLoop</tt>, as opposed to <tt>GradBasedInference</tt>, which is <tt>BatchInferenceLoop</tt>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that currently the code is not running distributed training in Horovod as we are still not running <tt>horovodrun</tt> or <tt>mpirun</tt> command from our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.inference import StochasticVariationalInference\n",
    "\n",
    "infr = DistributedGradBasedInference(inference_algorithm=StochasticVariationalInference(\n",
    "    model=m, posterior=q, num_samples=10, observed=[m.Y]))\n",
    "infr.run(Y=mx.nd.array(data, dtype='float64'), learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the resulting posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_mean = infr.params[q.mu.factor.mean].asscalar()\n",
    "mu_std = np.sqrt(infr.params[q.mu.factor.variance].asscalar())\n",
    "s_hat_mean = infr.params[q.s_hat.factor.mean].asscalar()\n",
    "s_hat_std = np.sqrt(infr.params[q.s_hat.factor.variance].asscalar())\n",
    "s_15 = np.log1p(np.exp(s_hat_mean - s_hat_std))\n",
    "s_50 = np.log1p(np.exp(s_hat_mean))\n",
    "s_85 = np.log1p(np.exp(s_hat_mean + s_hat_std))\n",
    "print('The mean and standard deviation of the mean parameter is %f(%f). ' % (mu_mean, mu_std))\n",
    "print('The 15th, 50th and 85th percentile of the variance parameter is %f, %f and %f.'%(s_15, s_50, s_85))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Horovod\n",
    "\n",
    "Currently, the only way to execute Horovod in MXFusion is via <tt>horovodrun</tt> or <tt>mpirun</tt> command from the system. Hence, we can first convert this notebook into Python file then execute the Python file with command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script getting_started-distributed.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run it on Horovod and allow distributed training, we should run <tt>horovodrun</tt> or <tt>mpirun</tt> from our system while specifying the number of processors. More details about running Horovod can be found [here](https://github.com/horovod/horovod/blob/master/docs/running.rst). A simple way to run it is with the format: <br><tt>horovodrun -np {number of processors} -H localhost:4 python {python file}</tt>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE : Please restart this notebook before executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 4 -H localhost:4 python getting_started-distributed.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
